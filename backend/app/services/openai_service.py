"""
AI Content Generation Service
‚úÖ PRIMARY: Google Gemini 2.0 Flash ($0.10/$0.40 per 1M tokens)
‚ö†Ô∏è FALLBACK: OpenAI GPT-4o-mini ($0.15/$0.60 per 1M tokens)
üíæ CACHING: Redis + Gemini prompt caching (90% discount)
üéØ QUALITY: Auto-regeneration for low-quality content (<0.60 score)

Decision: Gemini 2.0 Flash chosen for:
- 75% cost savings vs GPT-4o-mini
- Equal/better quality (8.3/10 vs 8.5/10)
- Faster response times
- 1M token context window
- FREE tier for development

See backend/AI_MODELS_CONFIG.md for full analysis
Updated: November 25, 2025
"""
from typing import Dict, Any, List, Optional
from openai import AsyncOpenAI
from openai import (
    APIError as OpenAIAPIError,
    RateLimitError as OpenAIRateLimitError,
    APIConnectionError,
    APITimeoutError as OpenAITimeoutError,
    AuthenticationError,
    BadRequestError
)
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
from app.config import settings
from app.utils.cache_manager import cache_manager
from app.utils.quality_scorer import quality_scorer, QualityScore
from app.exceptions import (
    AIServiceError,
    RateLimitError,
    InvalidAPIKeyError,
    AIModelNotFoundError,
    ContentPolicyViolationError,
    TokenLimitExceededError,
    AITimeoutError,
    NetworkError
)
import logging
import json
import time

logger = logging.getLogger(__name__)

class OpenAIService:
    """
    AI Content Generation Service
    PRIMARY: Gemini 2.0 Flash (cost-optimized)
    FALLBACK: GPT-4o-mini (quality backup)
    """
    _instance: Optional['OpenAIService'] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        # PRIMARY: Gemini 2.0 Flash - Use this by default
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.gemini_model = genai.GenerativeModel(settings.PRIMARY_TEXT_MODEL)
        self.gemini_premium_model = genai.GenerativeModel(settings.PREMIUM_TEXT_MODEL)
        
        # FALLBACK: OpenAI GPT-4o-mini - Use only on errors or quality issues
        self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.openai_model = settings.FALLBACK_TEXT_MODEL
        
        self.use_fallback = False  # Switch to OpenAI only if Gemini fails
        
        # Gemini cached system prompts (90% discount after 7-day TTL)
        # These are reused across all generations of the same content type
        self._cached_system_prompts: Dict[str, Any] = {}
    
    def _should_use_premium_model(self, user_tier: Optional[str] = None, content_complexity: str = "standard") -> bool:
        """
        Smart routing logic: Determine if premium model should be used
        
        Args:
            user_tier: User's subscription tier (free/hobby/pro/enterprise)
            content_complexity: Content complexity level (standard/complex/technical)
        
        Returns:
            bool: True if premium model (Gemini 2.5 Flash) should be used
        """
        from app.constants import SubscriptionPlan
        
        # Enterprise tier always gets premium model
        if user_tier == SubscriptionPlan.ENTERPRISE:
            return True
        
        # Pro tier gets premium for complex content
        if user_tier == SubscriptionPlan.PRO and content_complexity in ["complex", "technical"]:
            return True
        
        # Free/Hobby tiers always use standard model
        return False
    
    async def _generate_with_quality_check(
        self,
        system_prompt: str,
        user_prompt: str,
        max_tokens: int,
        use_premium: bool,
        content_type: str,
        user_id: Optional[str],
        metadata: Optional[Dict[str, Any]] = None,
        max_regenerations: int = 1
    ) -> Dict[str, Any]:
        """
        Generate content with automatic quality checking and regeneration
        
        If quality score < 0.60, automatically regenerates with premium model
        
        Args:
            system_prompt: System instructions
            user_prompt: User request
            max_tokens: Max tokens to generate
            use_premium: Whether to use premium model initially
            content_type: Type of content for quality scoring
            user_id: User ID for caching
            metadata: Metadata for quality scoring (keywords, target_length, etc.)
            max_regenerations: Maximum number of regeneration attempts
        
        Returns:
            Generation result with quality score
        """
        metadata = metadata or {}
        attempts = 0
        current_use_premium = use_premium
        
        while attempts <= max_regenerations:
            attempts += 1
            
            # Generate content
            result = await self._generate_with_ai(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=max_tokens,
                use_premium=current_use_premium,
                content_type=content_type,
                user_id=user_id
            )
            
            # Parse content for quality check
            try:
                if content_type in ['blog', 'email', 'product', 'ad']:
                    # JSON output - extract content field
                    output = json.loads(result['content'])
                    content_text = output.get('content', '') or str(output)
                else:
                    # Direct text
                    content_text = result['content']
            except (json.JSONDecodeError, KeyError) as e:
                # If JSON parsing fails, use raw content
                logger.debug(f"JSON parsing failed for quality check: {e}")
                content_text = result['content']
            
            # Score quality
            quality_score = quality_scorer.score_content(
                content=content_text,
                content_type=content_type,
                metadata=metadata
            )
            
            logger.info(f"Quality score: {quality_score.overall:.2f} (grade: {quality_score._get_grade()}) - Attempt {attempts}")
            
            # Check if regeneration needed
            if quality_scorer.should_regenerate(quality_score) and attempts <= max_regenerations:
                logger.warning(f"‚ö†Ô∏è Low quality score ({quality_score.overall:.2f}). Regenerating with premium model...")
                current_use_premium = True  # Upgrade to premium for retry
                continue
            
            # Quality acceptable or max attempts reached
            result['quality_score'] = quality_score.to_dict()
            result['regeneration_count'] = attempts - 1
            
            if quality_score.overall >= quality_scorer.EXCELLENT_THRESHOLD:
                logger.info(f"‚úÖ Excellent quality achieved: {quality_score.overall:.2f}")
            elif quality_score.overall >= quality_scorer.GOOD_THRESHOLD:
                logger.info(f"‚úÖ Good quality achieved: {quality_score.overall:.2f}")
            else:
                logger.warning(f"‚ö†Ô∏è Moderate quality: {quality_score.overall:.2f} (max attempts reached)")
            
            return result
        
        # Should never reach here, but return last attempt
        result['quality_score'] = quality_score.to_dict()
        result['regeneration_count'] = attempts - 1
        return result
    
    def _get_cached_system_content(self, content_type: str, system_prompt: str, use_premium: bool = False) -> Optional[Any]:
        """
        Get or create cached system prompt for Gemini
        Gemini prompt caching provides 90% discount on cached tokens
        
        Args:
            content_type: Type of content (blog, social, etc.)
            system_prompt: System prompt to cache
            use_premium: Whether using premium model
            
        Returns:
            Cached content object for Gemini or None
        """
        if not settings.ENABLE_PROMPT_CACHING:
            return None
        
        cache_key = f"{content_type}_{'premium' if use_premium else 'standard'}"
        
        # Return existing cached prompt if available
        if cache_key in self._cached_system_prompts:
            logger.debug(f"üíæ Using cached system prompt for {cache_key}")
            return self._cached_system_prompts[cache_key]
        
        # Create new cached content for Gemini
        try:
            cached_content = genai.caching.CachedContent.create(
                model=settings.PREMIUM_TEXT_MODEL if use_premium else settings.PRIMARY_TEXT_MODEL,
                contents=[{
                    'role': 'user',
                    'parts': [{'text': system_prompt}]
                }],
                ttl=settings.CACHE_TTL_SYSTEM_PROMPTS,  # 7 days
                display_name=f"{content_type}_system_prompt"
            )
            self._cached_system_prompts[cache_key] = cached_content
            logger.info(f"‚úÖ Created Gemini cached prompt for {cache_key} (90% discount for 7 days)")
            return cached_content
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to create Gemini cache for {cache_key}: {e}")
            return None
    
    async def _generate_with_ai(
        self, 
        system_prompt: str, 
        user_prompt: str, 
        max_tokens: int = 2000,
        use_premium: bool = False,
        content_type: str = "generic",
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate content with AI
        PRIMARY: Gemini 2.0 Flash (default) - 75% cheaper, excellent quality
        FALLBACK: GPT-4o-mini (on errors only)
        PREMIUM: Gemini 2.5 Flash (Enterprise tier, complex content)
        CACHING: Redis for generations + Gemini prompt caching (90% discount)
        
        Args:
            system_prompt: System instructions for the model
            user_prompt: User's actual request
            max_tokens: Maximum tokens to generate
            use_premium: If True, use premium model (Gemini 2.5 Flash)
            content_type: Type of content for caching
            user_id: User ID for personalized caching
        
        Returns:
            Dict with content, tokens used, model name, and cache stats
        """
        # Check Redis cache first for identical generations
        cache_key = None
        if user_id and settings.ENABLE_CACHE:
            cached_result = cache_manager.get_cached_generation(
                content_type=content_type,
                prompt=user_prompt,
                user_id=user_id
            )
            if cached_result:
                logger.info(f"üíæ Cache HIT: Returning cached {content_type} generation")
                cached_result['cached'] = True
                return cached_result
        
        start_time = time.time()
        
        # Try PRIMARY model first (Gemini 2.0 Flash)
        try:
            if not self.use_fallback:
                # Select model based on premium flag
                model_name = settings.PREMIUM_TEXT_MODEL if use_premium else settings.PRIMARY_TEXT_MODEL
                
                # Get cached system prompt (90% discount)
                cached_system = self._get_cached_system_content(
                    content_type=content_type,
                    system_prompt=system_prompt,
                    use_premium=use_premium
                )
                
                # Generate with Gemini using cached system prompt
                if cached_system:
                    # Use model with cached system prompt
                    model = genai.GenerativeModel.from_cached_content(cached_system)
                    response = model.generate_content(
                        user_prompt,
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=max_tokens,
                            temperature=0.7,
                        )
                    )
                else:
                    # Fallback to regular generation without caching
                    model = self.gemini_premium_model if use_premium else self.gemini_model
                    full_prompt = f"{system_prompt}\n\n{user_prompt}"
                    response = model.generate_content(
                        full_prompt,
                        generation_config=genai.types.GenerationConfig(
                            max_output_tokens=max_tokens,
                            temperature=0.7,
                        )
                    )
                
                # Clean response (remove markdown code blocks if present)
                content = response.text.strip()
                
                # Extract JSON from markdown code blocks
                if '```json' in content:
                    start = content.find('```json') + 7
                    end = content.find('```', start)
                    if end > start:
                        content = content[start:end].strip()
                elif '```' in content:
                    start = content.find('```') + 3
                    end = content.find('```', start)
                    if end > start:
                        content = content[start:end].strip()
                
                # Additional cleanup - find first { and last }
                if '{' in content and '}' in content:
                    first_brace = content.find('{')
                    last_brace = content.rfind('}')
                    content = content[first_brace:last_brace + 1]
                
                generation_time = time.time() - start_time
                logger.info(f"‚úÖ Generated with {model_name} in {generation_time:.2f}s (cached_prompt: {cached_system is not None})")
                
                result = {
                    'content': content,
                    'tokensUsed': 0,  # Gemini doesn't expose token count easily
                    'model': model_name,
                    'cached': False,
                    'cached_prompt': cached_system is not None,
                    'generation_time': generation_time
                }
                
                # Cache the result in Redis
                if user_id and settings.ENABLE_CACHE:
                    cache_manager.cache_generation(
                        content_type=content_type,
                        prompt=user_prompt,
                        result=result,
                        user_id=user_id,
                        ttl=settings.CACHE_TTL_GENERATIONS
                    )
                
                return result
                
        except google_exceptions.ResourceExhausted as e:
            # Gemini rate limit (429)
            logger.warning(f"‚ö†Ô∏è Gemini rate limited: {e}")
            self.use_fallback = True
        except google_exceptions.Unauthenticated as e:
            # Invalid API key (401)
            logger.error(f"‚ùå Gemini authentication failed: {e}")
            raise InvalidAPIKeyError("Gemini")
        except google_exceptions.InvalidArgument as e:
            # Invalid request (400) - possibly content policy or token limit
            logger.error(f"‚ùå Gemini invalid request: {e}")
            if "content" in str(e).lower() and "policy" in str(e).lower():
                raise ContentPolicyViolationError("Gemini", str(e))
            raise AIServiceError(str(e), "Gemini")
        except google_exceptions.DeadlineExceeded as e:
            # Timeout
            logger.warning(f"‚ö†Ô∏è Gemini timeout: {e}")
            self.use_fallback = True
        except google_exceptions.NotFound as e:
            # Model not found (404)
            logger.error(f"‚ùå Gemini model not found: {e}")
            raise AIModelNotFoundError(
                settings.PREMIUM_TEXT_MODEL if use_premium else settings.PRIMARY_TEXT_MODEL,
                "Gemini"
            )
        except Exception as e:
            # Unexpected Gemini error - switch to fallback
            logger.warning(f"‚ö†Ô∏è Gemini error, switching to fallback (GPT-4o-mini): {e}")
            self.use_fallback = True
        
        # FALLBACK to OpenAI GPT-4o-mini
        try:
            response = await self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=max_tokens
            )
            content = response.choices[0].message.content
            
            generation_time = time.time() - start_time
            logger.info(f"‚úÖ Generated with fallback {self.openai_model} in {generation_time:.2f}s")
            
            result = {
                'content': content,
                'tokensUsed': response.usage.total_tokens,
                'model': self.openai_model,
                'cached': False,
                'cached_prompt': False,
                'generation_time': generation_time
            }
            
            # Cache OpenAI result too
            if user_id and settings.ENABLE_CACHE:
                cache_manager.cache_generation(
                    content_type=content_type,
                    prompt=user_prompt,
                    result=result,
                    user_id=user_id,
                    ttl=settings.CACHE_TTL_GENERATIONS
                )
            
            return result
        except OpenAIRateLimitError as e:
            # OpenAI rate limit (429)
            logger.error(f"‚ùå Both Gemini and OpenAI rate limited: {e}")
            raise RateLimitError(
                service="OpenAI",
                retry_after=getattr(e, 'retry_after', None),
                limit="tokens or requests per minute"
            )
        except AuthenticationError as e:
            # Invalid API key (401)
            logger.error(f"‚ùå OpenAI authentication failed: {e}")
            raise InvalidAPIKeyError("OpenAI")
        except BadRequestError as e:
            # Invalid request (400) - token limit or content policy
            error_msg = str(e)
            logger.error(f"‚ùå OpenAI bad request: {error_msg}")
            if "maximum context length" in error_msg.lower():
                raise TokenLimitExceededError(
                    service="OpenAI",
                    requested=max_tokens,
                    limit=4096  # Default context window
                )
            elif "content" in error_msg.lower() and "policy" in error_msg.lower():
                raise ContentPolicyViolationError("OpenAI", error_msg)
            raise AIServiceError(error_msg, "OpenAI")
        except OpenAITimeoutError as e:
            # Timeout
            logger.error(f"‚ùå OpenAI timeout: {e}")
            raise AITimeoutError("OpenAI", timeout_seconds=60)
        except APIConnectionError as e:
            # Network error
            logger.error(f"‚ùå OpenAI connection error: {e}")
            raise NetworkError("OpenAI", str(e))
        except OpenAIAPIError as e:
            # Generic OpenAI API error (500, 502, 503)
            logger.error(f"‚ùå OpenAI API error: {e}")
            raise AIServiceError(str(e), "OpenAI")
        except Exception as e:
            logger.error(f"‚ùå Both Gemini and OpenAI failed with unexpected error: {e}", exc_info=True)
            raise AIServiceError(
                f"All AI services failed. Please try again later. Error: {str(e)}",
                "AI"
            )
    
    # ==================== CONTENT GENERATION ====================
    
    async def generate_blog_post(
        self,
        topic: str,
        keywords: List[str],
        tone: str,
        word_count: int,
        sections: Optional[List[str]] = None,
        user_tier: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Generate comprehensive blog post with smart model routing and caching"""
        
        # Smart routing: Use premium model for Enterprise or long-form content
        use_premium = self._should_use_premium_model(
            user_tier=user_tier,
            content_complexity="complex" if word_count > 1500 else "standard"
        )
        
        prompt = f"""Create a comprehensive, SEO-optimized blog post about: {topic}

Requirements:
- Tone: {tone}
- Target length: {word_count} words
- Keywords to include naturally: {', '.join(keywords)}
- Include these sections: {', '.join(sections) if sections else 'Introduction, Main Content (3-5 sections), Conclusion'}

Format as JSON:
{{
    "title": "engaging title with primary keyword",
    "metaDescription": "160 character SEO meta description",
    "content": "full article content in markdown",
    "headings": ["H2 heading 1", "H2 heading 2", ...],
    "wordCount": actual_word_count
}}

Make it engaging, informative, and naturally incorporate keywords. Use examples and data where relevant."""

        try:
            result = await self._generate_with_quality_check(
                system_prompt="You are an expert content writer specializing in SEO-optimized, engaging blog posts. Always return valid JSON.",
                user_prompt=prompt,
                max_tokens=4000,
                use_premium=use_premium,
                content_type="blog",
                user_id=user_id,
                metadata={
                    'keywords': keywords,
                    'target_length': word_count
                },
                max_regenerations=1
            )
            
            output = json.loads(result['content'])
            return {
                'output': output,
                'tokensUsed': result['tokensUsed'],
                'model': result['model'],
                'cached': result.get('cached', False),
                'cached_prompt': result.get('cached_prompt', False),
                'quality_score': result.get('quality_score'),
                'regeneration_count': result.get('regeneration_count', 0)
            }
        except Exception as e:
            logger.error(f"Error generating blog post: {e}")
            raise
    
    async def generate_social_media(
        self,
        content_description: str,
        platform: str,
        target_audience: str,
        tone: str,
        include_cta: bool = True,
        include_hashtags: bool = True,
        user_tier: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Generate platform-optimized social media captions with smart model routing and caching"""
        
        # Social media captions use standard model (quick, cost-effective)
        use_premium = self._should_use_premium_model(user_tier=user_tier, content_complexity="standard")
        
        platform_specs = {
            'linkedin': {'max_chars': 1300, 'style': 'professional, thought leadership'},
            'twitter': {'max_chars': 280, 'style': 'snappy, conversational'},
            'instagram': {'max_chars': 2200, 'style': 'engaging, storytelling'},
            'tiktok': {'max_chars': 300, 'style': 'casual, trendy, youth-focused'}
        }
        
        spec = platform_specs.get(platform.lower(), platform_specs['linkedin'])
        
        prompt = f"""Create 5 engaging {platform} captions for: {content_description}

Requirements:
- Platform: {platform} (max {spec['max_chars']} characters)
- Style: {spec['style']}
- Tone: {tone}
- Target audience: {target_audience}
- Include CTA: {include_cta}
- Include hashtags: {include_hashtags}

Format as JSON:
{{
    "captions": [
        {{"variation": 1, "text": "caption text", "length": char_count}},
        {{"variation": 2, "text": "caption text", "length": char_count}},
        ...
    ],
    "hashtags": ["#hashtag1", "#hashtag2", ...] (15-20 relevant hashtags),
    "emojiSuggestions": ["emoji1", "emoji2", ...],
    "engagementTips": "brief tips for maximizing engagement"
}}"""

        try:
            result = await self._generate_with_quality_check(
                system_prompt=f"You are a social media expert specializing in {platform} content. Always return valid JSON.",
                user_prompt=prompt,
                max_tokens=2000,
                use_premium=use_premium,
                content_type="social",
                user_id=user_id,
                metadata={'target_length': spec['max_chars']},
                max_regenerations=1
            )
            
            output = json.loads(result['content'])
            return {
                'output': output,
                'tokensUsed': result['tokensUsed'],
                'model': result['model'],
                'cached': result.get('cached', False),
                'cached_prompt': result.get('cached_prompt', False),
                'quality_score': result.get('quality_score'),
                'regeneration_count': result.get('regeneration_count', 0)
            }
        except Exception as e:
            logger.error(f"Error generating social media content: {e}")
            raise
    
    async def generate_email_campaign(
        self,
        campaign_type: str,
        product_service: str,
        target_audience: str,
        goal: str,
        tone: str,
        user_tier: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Generate email campaign content with smart model routing and caching"""
        
        # Email campaigns use standard model
        use_premium = self._should_use_premium_model(user_tier=user_tier, content_complexity="standard")
        
        prompt = f"""Create a compelling email campaign:

Type: {campaign_type}
Product/Service: {product_service}
Target Audience: {target_audience}
Goal: {goal}
Tone: {tone}

Format as JSON:
{{
    "subjectLines": ["subject 1", "subject 2", "subject 3"],
    "previewText": "preview text for inbox",
    "body": {{
        "intro": "opening paragraph",
        "benefits": ["benefit 1", "benefit 2", "benefit 3"],
        "mainContent": "main body content",
        "cta": "call to action text",
        "closing": "closing paragraph"
    }},
    "ctaButtonText": ["CTA option 1", "CTA option 2", "CTA option 3"],
    "bestSendTime": "recommended send time with reason"
}}"""

        try:
            result = await self._generate_with_quality_check(
                system_prompt="You are an email marketing expert. Always return valid JSON.",
                user_prompt=prompt,
                max_tokens=2000,
                use_premium=use_premium,
                content_type="email",
                user_id=user_id,
                metadata={'target_length': 500},
                max_regenerations=1
            )
            
            output = json.loads(result['content'])
            return {
                'output': output,
                'tokensUsed': result['tokensUsed'],
                'model': result['model'],
                'cached': result.get('cached', False),
                'cached_prompt': result.get('cached_prompt', False),
                'quality_score': result.get('quality_score'),
                'regeneration_count': result.get('regeneration_count', 0)
            }
        except Exception as e:
            logger.error(f"Error generating email campaign: {e}")
            raise
    
    async def generate_product_description(
        self,
        product_details: Dict[str, Any],
        target_customer: str,
        platform: str,
        include_seo: bool = True,
        user_tier: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Generate e-commerce product descriptions with smart model routing and caching"""
        
        # Product descriptions use standard model
        use_premium = self._should_use_premium_model(user_tier=user_tier, content_complexity="standard")
        
        prompt = f"""Create product descriptions for: {product_details.get('name', 'Product')}

Details: {json.dumps(product_details)}
Target Customer: {target_customer}
Platform: {platform}
Include SEO: {include_seo}

Format as JSON:
{{
    "shortDescription": "100 word description",
    "longDescription": "300 word detailed description",
    "bulletPoints": ["benefit 1", "benefit 2", ...] (5-7 key benefits),
    "seoTitle": "60 character title",
    "metaDescription": "160 character meta description",
    "categoryTags": ["tag1", "tag2", ...]
}}"""

        try:
            result = await self._generate_with_quality_check(
                system_prompt="You are an e-commerce copywriting expert. Always return valid JSON.",
                user_prompt=prompt,
                max_tokens=1500,
                use_premium=use_premium,
                content_type="product",
                user_id=user_id,
                metadata={'target_length': 400},
                max_regenerations=1
            )
            
            output = json.loads(result['content'])
            return {
                'output': output,
                'tokensUsed': result['tokensUsed'],
                'model': result['model'],
                'cached': result.get('cached', False),
                'cached_prompt': result.get('cached_prompt', False),
                'quality_score': result.get('quality_score'),
                'regeneration_count': result.get('regeneration_count', 0)
            }
        except Exception as e:
            logger.error(f"Error generating product description: {e}")
            raise
    
    async def generate_ad_copy(
        self,
        product_service: str,
        target_audience: str,
        platform: str,
        campaign_goal: str,
        user_tier: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Generate conversion-optimized ad copy with smart model routing and caching"""
        
        # Ad copy uses standard model (quick conversions)
        use_premium = self._should_use_premium_model(user_tier=user_tier, content_complexity="standard")
        
        prompt = f"""Create 3 high-converting ad copy variations:

Product/Service: {product_service}
Target Audience: {target_audience}
Platform: {platform}
Campaign Goal: {campaign_goal}

Format as JSON:
{{
    "adCopies": [
        {{
            "variation": 1,
            "uniqueAngle": "what makes this unique",
            "headlines": ["headline 1", "headline 2", "headline 3"],
            "body": "100-150 word ad copy",
            "ctas": ["CTA 1", "CTA 2", "CTA 3"],
            "emotionalTriggers": ["trigger 1", "trigger 2"]
        }},
        ...
    ],
    "estimatedCTR": "estimated click-through rate with reasoning"
}}"""

        try:
            result = await self._generate_with_quality_check(
                system_prompt="You are a conversion copywriting expert. Always return valid JSON.",
                user_prompt=prompt,
                max_tokens=2000,
                use_premium=use_premium,
                content_type="ad",
                user_id=user_id,
                metadata={'target_length': 150},
                max_regenerations=1
            )
            
            output = json.loads(result['content'])
            return {
                'output': output,
                'tokensUsed': result['tokensUsed'],
                'model': result['model'],
                'cached': result.get('cached', False),
                'cached_prompt': result.get('cached_prompt', False),
                'quality_score': result.get('quality_score'),
                'regeneration_count': result.get('regeneration_count', 0)
            }
        except Exception as e:
            logger.error(f"Error generating ad copy: {e}")
            raise
    
    async def generate_video_script(
        self,
        topic: str,
        duration_seconds: int,
        platform: str,
        target_audience: str,
        key_points: Optional[List[str]] = None,
        cta: str = "",
        user_tier: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Generate platform-optimized video script with smart model routing and caching"""
        
        # Video scripts use complex model for long-form content
        use_premium = self._should_use_premium_model(
            user_tier=user_tier,
            content_complexity="complex" if duration_seconds > 120 else "standard"
        )
        
        prompt = f"""Create a video script:

Topic: {topic}
Duration: {duration_seconds} seconds
Platform: {platform}
Target Audience: {target_audience}
Key Points: {', '.join(key_points) if key_points else 'Determine best points'}
CTA: {cta}

Format as JSON:
{{
    "hook": "first 5 seconds (retention optimized)",
    "script": [
        {{"timestamp": "0:00-0:05", "content": "hook content", "visualCue": "what to show"}},
        {{"timestamp": "0:05-0:15", "content": "main point 1", "visualCue": "b-roll suggestion"}},
        ...
    ],
    "ctaScript": "call to action script",
    "thumbnailTitles": ["title option 1", "title option 2", "title option 3"],
    "description": "platform-optimized description",
    "hashtags": ["#hashtag1", "#hashtag2", ...],
    "musicMood": "suggested music mood",
    "estimatedRetention": "estimated watch time retention with reasoning"
}}"""

        try:
            result = await self._generate_with_quality_check(
                system_prompt=f"You are a video script expert specializing in {platform}. Always return valid JSON.",
                user_prompt=prompt,
                max_tokens=3000,
                use_premium=use_premium,
                content_type="video",
                user_id=user_id,
                metadata={'target_length': duration_seconds * 2},  # ~2 words per second
                max_regenerations=1
            )
            
            output = json.loads(result['content'])
            return {
                'output': output,
                'tokensUsed': result['tokensUsed'],
                'model': result['model'],
                'cached': result.get('cached', False),
                'cached_prompt': result.get('cached_prompt', False),
                'quality_score': result.get('quality_score'),
                'regeneration_count': result.get('regeneration_count', 0)
            }
        except Exception as e:
            logger.error(f"Error generating video script: {e}")
            raise
    
    # ==================== QUALITY ANALYSIS ====================
    
    async def analyze_content_quality(self, content: str) -> Dict[str, Any]:
        """Analyze content quality metrics using primary AI model"""
        
        prompt = f"""Analyze this content for quality metrics:

{content}

Rate 0-100 for each metric and provide reasoning:

Format as JSON:
{{
    "readabilityScore": score,
    "grammarScore": score,
    "originalityScore": score,
    "engagementScore": score,
    "overallQuality": average_score,
    "improvements": ["suggestion 1", "suggestion 2", ...]
}}"""

        try:
            result = await self._generate_with_ai(
                system_prompt="You are a content quality analyst. Always return valid JSON.",
                user_prompt=prompt,
                max_tokens=500
            )
            
            output = json.loads(result['content'])
            return output
        except Exception as e:
            logger.error(f"Error analyzing content quality: {e}")
            raise

# Singleton instance
openai_service = OpenAIService()
