# üîç Feature: AI-Powered Fact-Checking

**Status:** ‚ùå **NOT IMPLEMENTED** (Critical Issue)  
**Priority:** üî¥ **CRITICAL** (Must implement within 2-3 weeks)  
**Owner:** Backend Team  
**Last Updated:** November 26, 2025

---

## üìã Executive Summary

### The Problem
**AI-generated content contains factual errors 30-40% of the time** (per research studies). Competitors like Jasper ($39-125/mo) and Copy.ai (~$49-99/mo) have NO fact-checking, leading to:
- Users publishing inaccurate statistics
- Brand reputation damage
- Manual fact-checking taking 1-2 hours per article
- Loss of trust in AI-generated content

### Our Solution
**AI-Powered Fact-Checking Layer** that automatically:
1. Extracts verifiable claims from generated content
2. Verifies each claim against credible sources (Wikipedia, .gov, .edu, academic journals)
3. Returns confidence scores (0-100%) + source citations
4. Flags unverified claims for user review before publishing

### Why This is Our #1 Differentiator
- ‚úÖ **ONLY Writesonic** has basic fact-checking (checks if sources exist, not accuracy)
- ‚ùå Jasper, Copy.ai, ContentBot, Rytr have **NO fact-checking**
- üí∞ Users pay $9-29/mo expecting this feature (it's our main USP)
- üéØ Can charge **$10-15/mo premium** for fact-checked content tier

---

## üö® CRITICAL ISSUE: Feature is Advertised but NOT Implemented

### Current Status in Codebase

**Blueprint Promise:**
```markdown
# AI-Powered Fact-Checking
Summarly verifies every factual claim with 90% accuracy using:
- Real-time source verification
- Cross-reference with credible databases
- Confidence scoring for each claim
```

**Actual Backend Implementation:**
```python
# backend/app/api/generate.py (Lines 204-207)
'factCheckResults': {
    'checked': False,      # ‚ùå Always returns FALSE
    'claims': [],          # ‚ùå Always EMPTY array
    'verificationTime': 0  # ‚ùå No verification happening
}
```

**Evidence from Code Analysis:**
```bash
# Search for fact-checking implementation
$ grep -r "def verify_fact" backend/app/services/
# Result: No matches found ‚ùå

$ grep -r "def check_fact" backend/app/services/
# Result: No matches found ‚ùå

$ grep -r "FactCheckService" backend/app/
# Result: No service class exists ‚ùå
```

### What EXISTS (Schemas Only):
‚úÖ Pydantic schemas defined in `backend/app/schemas/generation.py`:
```python
class FactCheckClaim(BaseModel):
    claim: str
    verified: bool
    source: Optional[str] = None
    confidence: float = Field(ge=0, le=1)

class FactCheckResults(BaseModel):
    checked: bool = False
    claims: List[FactCheckClaim] = Field(default_factory=list)
    verification_time: float = 0.0
```

‚úÖ User setting flag in `backend/app/schemas/user.py`:
```python
auto_fact_check: bool = Field(default=False, alias="autoFactCheck")
```

‚ùå **NO actual verification logic, NO API integrations, NO claim extraction**

### Immediate Risks
1. **False Advertising:** Users paying $9-29/mo for non-existent feature
2. **Trust Issues:** If users discover this, expect refunds + negative reviews
3. **Legal Risk:** Deceptive trade practices in some jurisdictions
4. **Competitive Disadvantage:** Can't compete with Writesonic (which HAS fact-checking)

---

## üéØ Implementation Roadmap

### Phase 1: Core Fact-Checking Service (Week 1-2)

#### 1.1 Create Fact-Checking Service Class

**File:** `backend/app/services/fact_check_service.py`

```python
"""
AI-Powered Fact-Checking Service
Verifies factual claims in generated content against credible sources

ARCHITECTURE:
1. Claim Extraction: Use AI to identify verifiable claims
2. Source Verification: Check against Google Fact Check API + credible databases
3. Confidence Scoring: Calculate reliability score (0-100%)
4. Citation Generation: Return source URLs + publication dates

EXTERNAL APIS:
- Google Fact Check Tools API (free tier: 1,000 calls/day)
- Wikipedia API (free, rate-limited)
- Academic databases: CrossRef, PubMed (free for metadata)
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import aiohttp
from app.schemas.generation import FactCheckClaim, FactCheckResults
from app.services.openai_service import OpenAIService
from app.config import settings

logger = logging.getLogger(__name__)

class FactCheckService:
    """AI-Powered fact verification service"""
    
    def __init__(self):
        self.openai_service = OpenAIService()
        self.google_fact_check_api_key = settings.GOOGLE_FACT_CHECK_API_KEY
        self.session: Optional[aiohttp.ClientSession] = None
        
        # Credible source domains (prioritized by trust level)
        self.credible_sources = {
            'tier_1': ['.gov', '.edu', 'who.int', 'cdc.gov', 'nih.gov'],
            'tier_2': ['wikipedia.org', 'britannica.com', 'nature.com', 'sciencedirect.com'],
            'tier_3': ['reuters.com', 'apnews.com', 'bbc.com', 'economist.com']
        }
    
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def verify_content(
        self,
        content: str,
        content_type: str,
        min_confidence: float = 0.7
    ) -> FactCheckResults:
        """
        Main entry point: Extract claims and verify them
        
        Args:
            content: Generated content to fact-check
            content_type: Type of content (blog, email, product, ad)
            min_confidence: Minimum confidence threshold (0-1)
        
        Returns:
            FactCheckResults with verified claims + sources
        """
        start_time = datetime.now()
        
        try:
            # Step 1: Extract verifiable claims from content
            claims_to_verify = await self._extract_claims(content, content_type)
            logger.info(f"Extracted {len(claims_to_verify)} claims for verification")
            
            if not claims_to_verify:
                logger.info("No verifiable claims found in content")
                return FactCheckResults(
                    checked=True,
                    claims=[],
                    verification_time=0.0
                )
            
            # Step 2: Verify each claim (parallel execution for speed)
            verified_claims = await asyncio.gather(
                *[self._verify_single_claim(claim) for claim in claims_to_verify],
                return_exceptions=True
            )
            
            # Filter out failed verifications
            valid_claims = [
                claim for claim in verified_claims 
                if isinstance(claim, FactCheckClaim) and claim.confidence >= min_confidence
            ]
            
            verification_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(
                f"Fact-check complete: {len(valid_claims)}/{len(claims_to_verify)} claims verified "
                f"(threshold: {min_confidence*100}%) in {verification_time:.2f}s"
            )
            
            return FactCheckResults(
                checked=True,
                claims=valid_claims,
                verification_time=verification_time
            )
        
        except Exception as e:
            logger.error(f"Fact-check failed: {e}", exc_info=True)
            return FactCheckResults(
                checked=False,
                claims=[],
                verification_time=(datetime.now() - start_time).total_seconds()
            )
    
    async def _extract_claims(self, content: str, content_type: str) -> List[str]:
        """
        Use AI to extract verifiable factual claims from content
        
        Examples of verifiable claims:
        - "Python was created in 1991" ‚úÖ
        - "45% of marketers use AI tools" ‚úÖ
        - "Gemini 2.0 Flash costs $0.10 per 1M tokens" ‚úÖ
        
        NOT verifiable (opinions):
        - "Python is the best programming language" ‚ùå
        - "AI will change the world" ‚ùå
        """
        extraction_prompt = f"""Extract ONLY verifiable factual claims from this {content_type} content.

RULES:
1. Only extract claims with specific facts (numbers, dates, names, statistics)
2. Ignore opinions, predictions, or subjective statements
3. Each claim should be independently verifiable
4. Return as JSON array of strings

CONTENT:
{content[:3000]}

OUTPUT FORMAT:
{{
    "claims": [
        "Python was created by Guido van Rossum in 1991",
        "The global AI market is expected to reach $190 billion by 2025",
        "Gemini 2.0 Flash costs $0.10 per 1M input tokens"
    ]
}}
"""
        
        result = await self.openai_service._generate_with_ai(
            system_prompt="You are a fact-checking assistant that extracts verifiable claims.",
            user_prompt=extraction_prompt,
            max_tokens=1000,
            content_type="fact_extraction",
            use_premium=False
        )
        
        try:
            import json
            claims_data = json.loads(result['content'])
            claims = claims_data.get('claims', [])
            return claims[:10]  # Limit to 10 claims to avoid excessive API calls
        except Exception as e:
            logger.error(f"Failed to parse extracted claims: {e}")
            return []
    
    async def _verify_single_claim(self, claim: str) -> FactCheckClaim:
        """
        Verify a single claim using multiple sources
        
        VERIFICATION STRATEGY:
        1. Google Fact Check API (primary)
        2. Wikipedia API (secondary)
        3. Academic databases (for scientific claims)
        4. News APIs (for recent events)
        """
        logger.debug(f"Verifying claim: {claim}")
        
        # Run all verification methods in parallel
        results = await asyncio.gather(
            self._check_google_fact_check(claim),
            self._check_wikipedia(claim),
            self._check_web_search(claim),
            return_exceptions=True
        )
        
        # Aggregate results from multiple sources
        verified = False
        best_source = None
        max_confidence = 0.0
        
        for result in results:
            if isinstance(result, dict) and result.get('verified'):
                verified = True
                if result['confidence'] > max_confidence:
                    max_confidence = result['confidence']
                    best_source = result['source']
        
        return FactCheckClaim(
            claim=claim,
            verified=verified,
            source=best_source,
            confidence=max_confidence
        )
    
    async def _check_google_fact_check(self, claim: str) -> Dict[str, Any]:
        """
        Verify claim using Google Fact Check Tools API
        
        API DOCS: https://developers.google.com/fact-check/tools/api/reference/rest
        FREE TIER: 1,000 calls/day
        """
        if not self.google_fact_check_api_key:
            logger.warning("Google Fact Check API key not configured")
            return {'verified': False, 'confidence': 0.0, 'source': None}
        
        url = "https://factchecktools.googleapis.com/v1alpha1/claims:search"
        params = {
            'query': claim,
            'key': self.google_fact_check_api_key,
            'languageCode': 'en'
        }
        
        try:
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('claims'):
                        # Found fact-check results
                        first_claim = data['claims'][0]
                        claim_review = first_claim.get('claimReview', [{}])[0]
                        
                        rating = claim_review.get('textualRating', '').lower()
                        source_url = claim_review.get('url')
                        publisher = claim_review.get('publisher', {}).get('name')
                        
                        # Map textual ratings to confidence scores
                        confidence_map = {
                            'true': 0.95,
                            'mostly true': 0.85,
                            'half true': 0.60,
                            'mostly false': 0.30,
                            'false': 0.05,
                            'unverified': 0.40
                        }
                        
                        confidence = confidence_map.get(rating, 0.50)
                        verified = confidence >= 0.60
                        
                        return {
                            'verified': verified,
                            'confidence': confidence,
                            'source': f"{publisher} - {source_url}" if source_url else publisher
                        }
                
                return {'verified': False, 'confidence': 0.0, 'source': None}
        
        except Exception as e:
            logger.error(f"Google Fact Check API error: {e}")
            return {'verified': False, 'confidence': 0.0, 'source': None}
    
    async def _check_wikipedia(self, claim: str) -> Dict[str, Any]:
        """
        Verify claim against Wikipedia (high-trust source)
        
        APPROACH:
        1. Search Wikipedia for relevant articles
        2. Check if claim appears in article text
        3. Return confidence based on match quality
        """
        url = "https://en.wikipedia.org/w/api.php"
        params = {
            'action': 'query',
            'list': 'search',
            'srsearch': claim,
            'format': 'json',
            'utf8': 1
        }
        
        try:
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    search_results = data.get('query', {}).get('search', [])
                    
                    if search_results:
                        # Found relevant Wikipedia articles
                        first_result = search_results[0]
                        title = first_result['title']
                        snippet = first_result.get('snippet', '')
                        
                        # Check if key terms from claim appear in snippet
                        claim_words = set(claim.lower().split())
                        snippet_words = set(snippet.lower().split())
                        overlap = len(claim_words & snippet_words) / len(claim_words)
                        
                        confidence = min(0.85, overlap * 1.2)  # Cap at 85% for Wikipedia
                        verified = confidence >= 0.60
                        
                        return {
                            'verified': verified,
                            'confidence': confidence,
                            'source': f"Wikipedia - {title}"
                        }
            
            return {'verified': False, 'confidence': 0.0, 'source': None}
        
        except Exception as e:
            logger.error(f"Wikipedia API error: {e}")
            return {'verified': False, 'confidence': 0.0, 'source': None}
    
    async def _check_web_search(self, claim: str) -> Dict[str, Any]:
        """
        Verify claim using web search (fallback method)
        
        APPROACH:
        1. Use Bing/Google Search API to find credible sources
        2. Check if claim appears in .gov, .edu, or high-trust domains
        3. Return confidence based on source credibility
        """
        # TODO: Implement Bing Search API integration
        # For now, return neutral result
        return {'verified': False, 'confidence': 0.50, 'source': 'Web search pending'}

# Global service instance
fact_check_service = FactCheckService()
```

#### 1.2 Update Generation Endpoints to Use Fact-Checking

**File:** `backend/app/api/generate.py`

**Changes Required:**
```python
# Add import
from app.services.fact_check_service import fact_check_service

# Update blog generation endpoint (around line 200)
async def generate_blog(request: BlogGenerationRequest, user=Depends(get_current_user)):
    # ... existing generation code ...
    
    # AFTER content generation, BEFORE saving to Firestore:
    fact_check_results_dict = {'checked': False, 'claims': [], 'verificationTime': 0}
    
    # Check if user has auto_fact_check enabled
    user_settings = await firebase_service.get_user_settings(user_id)
    if user_settings.get('autoFactCheck', False):
        logger.info("Running fact-check on generated blog post...")
        
        async with fact_check_service as fc:
            fact_check_results = await fc.verify_content(
                content=output.get('content', ''),
                content_type='blog',
                min_confidence=0.70  # 70% confidence threshold
            )
            
            fact_check_results_dict = {
                'checked': fact_check_results.checked,
                'claims': [
                    {
                        'claim': c.claim,
                        'verified': c.verified,
                        'source': c.source,
                        'confidence': c.confidence
                    }
                    for c in fact_check_results.claims
                ],
                'verificationTime': fact_check_results.verification_time
            }
            
            logger.info(
                f"Fact-check complete: {len(fact_check_results.claims)} claims verified "
                f"in {fact_check_results.verification_time:.2f}s"
            )
    
    # Update generation_data with actual fact-check results
    generation_data['factCheckResults'] = fact_check_results_dict
    
    # ... rest of existing code ...
```

#### 1.3 Add Configuration for APIs

**File:** `backend/app/config.py`

```python
class Settings(BaseSettings):
    # ... existing settings ...
    
    # Fact-Checking APIs
    GOOGLE_FACT_CHECK_API_KEY: str = Field(default="", env="GOOGLE_FACT_CHECK_API_KEY")
    ENABLE_FACT_CHECKING: bool = Field(default=True, env="ENABLE_FACT_CHECKING")
    FACT_CHECK_MIN_CONFIDENCE: float = Field(default=0.70, env="FACT_CHECK_MIN_CONFIDENCE")
```

**File:** `.env`

```bash
# Fact-Checking Configuration
GOOGLE_FACT_CHECK_API_KEY=your_api_key_here
ENABLE_FACT_CHECKING=true
FACT_CHECK_MIN_CONFIDENCE=0.70
```

---

### Phase 2: Frontend Integration (Week 2-3)

#### 2.1 Display Fact-Check Results in UI

**Show fact-check status in generation results:**
```typescript
// Display verified claims with confidence scores
{factCheckResults.checked && (
  <FactCheckPanel>
    <h3>Fact-Check Results ‚úÖ</h3>
    <p>{factCheckResults.claims.length} claims verified</p>
    
    {factCheckResults.claims.map((claim, idx) => (
      <ClaimCard key={idx} verified={claim.verified}>
        <p>{claim.claim}</p>
        <Badge confidence={claim.confidence}>
          {(claim.confidence * 100).toFixed(0)}% confident
        </Badge>
        {claim.source && (
          <Source href={claim.source} target="_blank">
            Source: {claim.source}
          </Source>
        )}
      </ClaimCard>
    ))}
  </FactCheckPanel>
)}
```

#### 2.2 Add User Setting Toggle

**Allow users to enable/disable auto fact-checking:**
```typescript
// Settings page
<SettingToggle
  label="Auto Fact-Check Content"
  description="Automatically verify factual claims in generated content (adds 5-10 seconds)"
  checked={userSettings.autoFactCheck}
  onChange={(checked) => updateSettings({ autoFactCheck: checked })}
/>
```

---

### Phase 3: Premium Tier Gating (Week 3)

#### 3.1 Make Fact-Checking a Premium Feature

**Business Logic:**
- **Free Tier:** NO fact-checking
- **Hobby ($9/mo):** Up to 10 fact-checked generations/month
- **Pro ($29/mo):** UNLIMITED fact-checking
- **Enterprise:** UNLIMITED + priority fact-checking (faster processing)

**Implementation:**
```python
# backend/app/api/generate.py
async def generate_blog(request: BlogGenerationRequest, user=Depends(get_current_user)):
    # Check subscription tier
    subscription = await firebase_service.get_user_subscription(user_id)
    tier = subscription.get('tier', 'free')
    
    # Check fact-checking quota
    if tier == 'free':
        # Free users can't use fact-checking
        fact_check_enabled = False
    elif tier == 'hobby':
        # Hobby users: 10 fact-checks/month
        fact_check_count = await firebase_service.get_fact_check_count(user_id)
        if fact_check_count >= 10:
            logger.warning(f"User {user_id} exceeded Hobby tier fact-check limit (10/month)")
            fact_check_enabled = False
        else:
            fact_check_enabled = user_settings.get('autoFactCheck', False)
    else:
        # Pro/Enterprise: unlimited
        fact_check_enabled = user_settings.get('autoFactCheck', False)
    
    # ... run fact-checking if enabled ...
```

---

## üèÜ Competitive Differentiation

### What Makes Our Fact-Checking Unique

| Feature | Jasper | Copy.ai | Writesonic | ContentBot | Rytr | **Summarly** |
|---------|--------|---------|------------|------------|------|------------|
| **Fact-Checking** | ‚ùå No | ‚ùå No | ‚úÖ Basic (source exists) | ‚ùå No | ‚ùå No | ‚úÖ **Deep verification** |
| **Confidence Scores** | N/A | N/A | ‚ùå No | N/A | N/A | ‚úÖ **0-100% per claim** |
| **Source Citations** | N/A | N/A | ‚úÖ Yes | N/A | N/A | ‚úÖ **With URLs + publishers** |
| **Multi-Source Verification** | N/A | N/A | ‚ùå No (single source) | N/A | N/A | ‚úÖ **Google + Wikipedia + Web** |
| **Credibility Tiers** | N/A | N/A | ‚ùå No | N/A | N/A | ‚úÖ **.gov > .edu > Wikipedia** |

### Why We Win

1. **ONLY Writesonic has fact-checking** (basic version checks if sources exist, not accuracy)
2. **Our approach is MORE RIGOROUS:** Multi-source verification with confidence scores
3. **Transparent sourcing:** Show users WHERE facts came from (build trust)
4. **Tiered credibility:** Prioritize .gov/.edu over generic websites
5. **Saves 1-2 hours** of manual fact-checking per article

---

## üìä Success Metrics

### Implementation Metrics
- [ ] `FactCheckService` class created with 90%+ test coverage
- [ ] Google Fact Check API integrated (1,000 free calls/day)
- [ ] Wikipedia API integrated (fallback verification)
- [ ] Web search API integrated (secondary fallback)
- [ ] All 6 content types (blog, social, email, product, ad, video) support fact-checking
- [ ] Frontend UI displays fact-check results with confidence scores
- [ ] User settings page has "Auto Fact-Check" toggle
- [ ] Premium tier gating: Free (0), Hobby (10/mo), Pro (unlimited)

### Business Metrics (3 months post-launch)
- **Target:** 40% of Pro users enable auto fact-check
- **Target:** Average confidence score ‚â• 75% across all verified claims
- **Target:** Fact-check time ‚â§ 10 seconds per generation
- **Target:** User satisfaction: "Content accuracy improved" ‚â• 80% (survey)
- **Target:** Conversion increase: 15-20% Free ‚Üí Hobby upgrade ("I need fact-checking")

### Quality Metrics
- **Accuracy:** False positive rate ‚â§ 10% (claims marked "verified" that are actually false)
- **Recall:** False negative rate ‚â§ 15% (claims marked "unverified" that are actually true)
- **Speed:** P95 fact-check latency ‚â§ 15 seconds
- **Coverage:** ‚â• 70% of factual claims successfully extracted and verified

---

## üîß Technical Architecture

### System Design

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Generates Content                        ‚îÇ
‚îÇ              (Blog post, Email, Product description)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   OpenAI Service (Gemini 2.0 Flash)              ‚îÇ
‚îÇ                  Generates content with quality check            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Check User Setting  ‚îÇ
                    ‚îÇ   autoFactCheck=true? ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ YES
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Fact Check Service                            ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  Step 1: Extract Claims (AI)                                     ‚îÇ
‚îÇ  ‚îú‚îÄ Use OpenAI to identify verifiable claims                     ‚îÇ
‚îÇ  ‚îú‚îÄ Filter out opinions, predictions, subjective statements      ‚îÇ
‚îÇ  ‚îî‚îÄ Return 5-10 specific factual claims                          ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  Step 2: Verify Claims (Parallel)                                ‚îÇ
‚îÇ  ‚îú‚îÄ Google Fact Check API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ  ‚îú‚îÄ Wikipedia API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                             ‚îÇ
‚îÇ  ‚îî‚îÄ Web Search API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                             ‚îÇ
‚îÇ                                     ‚îú‚îÄ‚îÄ Aggregate Results         ‚îÇ
‚îÇ                                     ‚îú‚îÄ‚îÄ Calculate Confidence      ‚îÇ
‚îÇ                                     ‚îî‚îÄ‚îÄ Select Best Source        ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  Step 3: Return Results                                          ‚îÇ
‚îÇ  ‚îî‚îÄ Claims array with verified status, confidence, sources       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Save to Firestore                             ‚îÇ
‚îÇ  {                                                                ‚îÇ
‚îÇ    "factCheckResults": {                                          ‚îÇ
‚îÇ      "checked": true,                                             ‚îÇ
‚îÇ      "claims": [                                                  ‚îÇ
‚îÇ        {                                                          ‚îÇ
‚îÇ          "claim": "Python was created in 1991",                   ‚îÇ
‚îÇ          "verified": true,                                        ‚îÇ
‚îÇ          "source": "Wikipedia - Python (programming language)",   ‚îÇ
‚îÇ          "confidence": 0.92                                       ‚îÇ
‚îÇ        }                                                          ‚îÇ
‚îÇ      ],                                                           ‚îÇ
‚îÇ      "verificationTime": 8.3                                      ‚îÇ
‚îÇ    }                                                              ‚îÇ
‚îÇ  }                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Display in Frontend                           ‚îÇ
‚îÇ  ‚úÖ Fact-Check Results (8 claims verified in 8.3s)               ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Python was created in 1991             [92% ‚úì]  ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ    Source: Wikipedia - Python (programming)         ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ ‚ö†Ô∏è  Global AI market will reach $500B      [45% ?] ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ    Source: Unverified prediction                    ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### API Integration Details

#### Google Fact Check Tools API
- **Endpoint:** `https://factchecktools.googleapis.com/v1alpha1/claims:search`
- **Free Tier:** 1,000 calls/day
- **Rate Limit:** 100 requests per 100 seconds per user
- **Response:** ClaimReview objects with publisher, rating, URL
- **Setup:** Get API key from Google Cloud Console

#### Wikipedia API
- **Endpoint:** `https://en.wikipedia.org/w/api.php`
- **Free:** Unlimited (rate-limited to 200 req/sec)
- **Response:** Search results with article titles, snippets
- **Reliability:** High trust (but not always 100% accurate)

#### Web Search API (Optional)
- **Option 1:** Bing Search API (5,000 free calls/month)
- **Option 2:** Google Custom Search API (100 free calls/day)
- **Use Case:** Verify recent news, check credible domains (.gov, .edu)

---

## üß™ Testing Strategy

### Unit Tests

**File:** `backend/tests/test_fact_check_service.py`

```python
import pytest
from app.services.fact_check_service import FactCheckService

@pytest.mark.asyncio
async def test_extract_claims_from_blog():
    """Test that AI correctly extracts verifiable claims"""
    content = """
    Python was created by Guido van Rossum in 1991. It's now one of the most 
    popular programming languages. According to Stack Overflow's 2023 survey, 
    45% of developers use Python regularly.
    """
    
    async with FactCheckService() as fc:
        claims = await fc._extract_claims(content, 'blog')
    
    assert len(claims) >= 2
    assert any('1991' in claim for claim in claims)
    assert any('45%' in claim or '45 percent' in claim for claim in claims)

@pytest.mark.asyncio
async def test_verify_known_fact():
    """Test that service verifies a well-known fact"""
    claim = "Python was created by Guido van Rossum in 1991"
    
    async with FactCheckService() as fc:
        result = await fc._verify_single_claim(claim)
    
    assert result.verified is True
    assert result.confidence >= 0.70
    assert result.source is not None

@pytest.mark.asyncio
async def test_flag_unverifiable_claim():
    """Test that service flags claims that can't be verified"""
    claim = "XYZ Company will dominate the market by 2030"  # Prediction, not verifiable
    
    async with FactCheckService() as fc:
        result = await fc._verify_single_claim(claim)
    
    assert result.confidence < 0.60  # Low confidence for predictions

@pytest.mark.asyncio
async def test_end_to_end_fact_check():
    """Test full fact-checking workflow"""
    content = """
    The COVID-19 pandemic began in late 2019 in Wuhan, China. According to WHO, 
    there were over 700 million confirmed cases worldwide as of 2023.
    """
    
    async with FactCheckService() as fc:
        results = await fc.verify_content(content, 'blog', min_confidence=0.70)
    
    assert results.checked is True
    assert len(results.claims) > 0
    assert results.verification_time < 30.0  # Should complete in <30 seconds
```

### Integration Tests

**Test fact-checking in generation endpoint:**
```python
@pytest.mark.asyncio
async def test_blog_generation_with_fact_check(client, test_user):
    """Test that blog generation includes fact-check results"""
    
    # Enable auto fact-check for test user
    await firebase_service.update_user_settings(
        test_user['uid'],
        {'autoFactCheck': True}
    )
    
    response = await client.post(
        "/api/v1/generate/blog",
        json={
            "topic": "History of Python Programming Language",
            "keywords": ["Python", "Guido van Rossum", "1991"],
            "tone": "professional",
            "length": "medium"
        },
        headers={"Authorization": f"Bearer {test_user['token']}"}
    )
    
    assert response.status_code == 200
    data = response.json()
    
    # Check fact-check results exist
    assert 'fact_check_results' in data
    assert data['fact_check_results']['checked'] is True
    assert len(data['fact_check_results']['claims']) > 0
    
    # Check that at least one claim about Python's creation is verified
    claims = data['fact_check_results']['claims']
    python_claims = [c for c in claims if '1991' in c['claim'] or 'Guido' in c['claim']]
    assert len(python_claims) > 0
    assert python_claims[0]['verified'] is True
```

---

## üìù Implementation Checklist

### Week 1: Core Service (Backend)
- [ ] Create `FactCheckService` class in `backend/app/services/fact_check_service.py`
- [ ] Implement `_extract_claims()` method (use OpenAI to identify verifiable claims)
- [ ] Implement `_verify_single_claim()` method (multi-source verification)
- [ ] Integrate Google Fact Check Tools API
- [ ] Integrate Wikipedia API
- [ ] Add configuration to `config.py` and `.env`
- [ ] Write unit tests (90%+ coverage)
- [ ] Test with real-world content samples

### Week 2: API Integration
- [ ] Update `backend/app/api/generate.py` to call `fact_check_service`
- [ ] Add fact-checking to blog generation endpoint
- [ ] Add fact-checking to email generation endpoint
- [ ] Add fact-checking to product description endpoint
- [ ] Add fact-checking to ad copy endpoint
- [ ] Add fact-checking quota checks (Free: 0, Hobby: 10/mo, Pro: unlimited)
- [ ] Write integration tests
- [ ] Load testing (verify <15 sec P95 latency)

### Week 3: Frontend + Premium Gating
- [ ] Create `FactCheckPanel` component to display results
- [ ] Show confidence scores + source citations in UI
- [ ] Add "Auto Fact-Check" toggle in user settings page
- [ ] Add fact-check quota display (e.g., "7/10 fact-checks used this month")
- [ ] Add upgrade prompt for Free users: "Upgrade to Hobby for fact-checking"
- [ ] Update pricing page: Highlight fact-checking as premium feature
- [ ] Write E2E tests (Cypress/Playwright)

### Week 4: Polish + Launch
- [ ] Performance optimization (parallel API calls, caching)
- [ ] Error handling (graceful degradation if APIs fail)
- [ ] Monitoring/logging (track fact-check success rate, latency)
- [ ] Documentation (API docs, user guide)
- [ ] Marketing materials (blog post, video demo)
- [ ] Beta testing with 10-20 Pro users
- [ ] Public launch announcement

---

## üöÄ Go-to-Market Strategy

### Messaging
**Primary:** "The ONLY AI content tool that verifies every fact automatically"

**Taglines:**
- "Never publish fake AI-generated facts again"
- "AI + Fact-Checking = Content you can trust"
- "Jasper writes fast. Summarly writes **accurate**."

### Target Audiences
1. **Content Marketers** - Published incorrect stats, need accuracy
2. **Journalists/Publishers** - Reputation depends on factual accuracy
3. **Academic Writers** - Citations required for credibility
4. **Legal/Medical Professionals** - Accuracy is legally critical

### Launch Campaign
1. **Comparison Blog Post:** "Jasper vs. Summarly: Why Fact-Checking Matters"
2. **Video Demo:** Show side-by-side comparison (Jasper publishes fake stat, Summarly flags it)
3. **Case Study:** "How Agency X saved 10 hours/week on fact-checking"
4. **Reddit AMA:** r/content_marketing - "We built AI fact-checking, AMA"
5. **ProductHunt Launch:** Emphasize unique fact-checking feature

---

## üí∞ Revenue Impact

### Pricing Strategy
- **Free Tier:** NO fact-checking (drive upgrades)
- **Hobby ($9/mo):** 10 fact-checked generations/month
- **Pro ($29/mo):** UNLIMITED fact-checking ‚Üê **Primary upsell**
- **Enterprise (custom):** Priority fact-checking (faster processing)

### Projected Conversions
- **Assumption:** 30% of users care about accuracy (content marketers, journalists)
- **Free ‚Üí Hobby:** 15-20% conversion driven by fact-checking need
- **Hobby ‚Üí Pro:** 25-30% conversion once they hit 10/month limit

### Example Revenue Model (1,000 users)
- Free Tier: 500 users ‚Üí $0
- Hobby ($9/mo): 300 users ‚Üí $2,700/mo ‚Üí **$32,400/year**
- Pro ($29/mo): 180 users ‚Üí $5,220/mo ‚Üí **$62,640/year**
- Enterprise ($99/mo): 20 users ‚Üí $1,980/mo ‚Üí **$23,760/year**

**Total Revenue:** $118,800/year  
**With Fact-Checking as differentiator:** +25% conversion = **$148,500/year (+$29,700)**

---

## ‚ö†Ô∏è Risks & Mitigation

### Risk 1: API Costs Too High
**Impact:** Google Fact Check API costs $5 per 1,000 calls after free tier  
**Mitigation:**
- Use free tier (1,000/day) initially
- Cache verification results (same claim verified once = reuse for 7 days)
- Only fact-check Pro tier users (limit Free/Hobby to reduce costs)

### Risk 2: False Positives (Marking True Claims as Unverified)
**Impact:** Users lose trust if accurate content flagged as "unverified"  
**Mitigation:**
- Set conservative confidence thresholds (70%+)
- Show confidence scores transparently (let users judge)
- Allow users to manually approve claims

### Risk 3: Slow Fact-Checking (>30 seconds)
**Impact:** Users abandon generation if it takes too long  
**Mitigation:**
- Parallel API calls (Google + Wikipedia simultaneously)
- Limit to 10 claims per generation
- Show progress indicator: "Verifying claim 3 of 8..."

### Risk 4: Competitors Copy Feature
**Impact:** Jasper/Copy.ai add fact-checking after seeing our success  
**Mitigation:**
- Move fast (implement in 3 weeks, launch publicly)
- Build moat: Multi-source verification (not just single API)
- Patent application for "AI-powered multi-source fact verification system"

---

## üìö Resources & References

### API Documentation
- [Google Fact Check Tools API](https://developers.google.com/fact-check/tools/api)
- [Wikipedia API](https://www.mediawiki.org/wiki/API:Main_page)
- [Bing Search API](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api)

### Research Papers
- "Automated Fact-Checking: Task Formulations, Methods and Future Directions" (ACL 2021)
- "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking" (EMNLP 2017)

### Competitive Analysis
- Writesonic Fact-Checking: Basic source existence check, no confidence scores
- Google's Fact Check Explorer: Manual tool, not automated

---

**Last Updated:** November 26, 2025  
**Next Review:** December 10, 2025 (after Phase 1 completion)  
**Owner:** Backend Team + Product Manager  
**Status:** ‚ùå **NOT IMPLEMENTED - CRITICAL PRIORITY**
